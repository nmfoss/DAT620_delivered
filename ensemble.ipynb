{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import modeling_tools as mt\n",
    "import murmurhash as mhash\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = pd.read_pickle('../../data/labeled_data_pos_ont_nn.pkl')\n",
    "drop_trash = (labeled_data['Aggr.Label'] < 90) & (labeled_data['NN_bool'] == False)\n",
    "labeled_data = labeled_data[drop_trash]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_features = ['Adjective', 'Verb', 'Noun']\n",
    "ont_features = ['Underspecified', 'Artifact', 'Object', 'Group', 'Human', 'Natural', 'LanguageRepresentation', 'Living', 'GeopoliticalPlace', 'BodyPart', 'Instrument', 'Place', '3rdOrderEntity', 'Mental', 'Purpose', 'Social', 'Institution', 'Plant', 'Imagerepresentation', 'Creature', 'Animal', 'Comestible', 'Quantity', 'Building', 'Substance', 'Part', 'Property', 'BoundedEvent', 'Agentive', 'Communication', 'Garment', 'Furniture', 'Vehicle', '1stOrderEntity', 'Covering', 'Liquid', 'Time', 'UnboundedEvent', 'Physical', 'Dynamic', 'Domain', 'Existence', 'Location', 'Manner', 'Container', 'Condition', 'Static', '2ndOrderEntity', 'Phenomenal', 'MoneyRepresentation', 'Experience', 'Relation', 'Form', 'Representation', 'Stimulating', 'Colour', 'Cause', 'Occupation', 'Possession', 'Artwork', 'Software']\n",
    "numerical_features = ['Raw_len', 'Raw_word_count']\n",
    "categorical_features = ['Label', 'Aggr.Label', 'Source', 'CoderId']\n",
    "text_features = ['Lemma_stripped', 'Raw', 'Lemma']\n",
    "fastt_features = ['Raw_FT_mean', 'Lemma_FT_mean']\n",
    "features = pos_features + ont_features + numerical_features + text_features + fastt_features + categorical_features\n",
    "\n",
    "fastt = 'Raw_FT_mean'\n",
    "corpus = 'Lemma'\n",
    "target = 'Aggr.Label'\n",
    "#target = 'Label'\n",
    "\n",
    "\n",
    "train_X, validation_X, train_y, validation_y = train_test_split(\n",
    "    labeled_data[features],\n",
    "    labeled_data[target],\n",
    "    test_size=0.33,\n",
    "    random_state=1,\n",
    "    stratify=labeled_data[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleClassifier(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, binary_clf, binary_clf_params, bucket_clf, bucket_clf_params, judge):\n",
    "        self.binary_clf = binary_clf\n",
    "        self.binary_clf_params = binary_clf_params\n",
    "        self.bucket_clf = bucket_clf\n",
    "        self.bucket_clf_params = bucket_clf_params\n",
    "        self.judge = judge\n",
    "        \n",
    "    def get_buckets(self, counted):\n",
    "        counted_sum = sum(counted.values())\n",
    "        buckets = []\n",
    "        index = 0\n",
    "        for c, v in counted.most_common():\n",
    "            if index == 0:\n",
    "                sum_left = counted_sum\n",
    "                sum_bucket = 0\n",
    "                new_bucket = []\n",
    "\n",
    "            new_bucket.append(c)\n",
    "            sum_bucket += v\n",
    "            sum_left -= v\n",
    "\n",
    "            if sum_bucket >= sum_left:\n",
    "                buckets.append(new_bucket)\n",
    "                new_bucket = []\n",
    "                sum_bucket = 0\n",
    "            index += 1\n",
    "            if index == len(counted) and len(new_bucket) > 0:\n",
    "                buckets.append(new_bucket)\n",
    "        return buckets\n",
    "    \n",
    "    def get_vs_all(self, buckets):\n",
    "        bucket_vs_all = []\n",
    "        for l_index in range(len(buckets)):\n",
    "            vs_all = []\n",
    "            if l_index+1 < len(buckets):\n",
    "                for r_index in range(l_index+1, len(buckets)):\n",
    "                    vs_all += buckets[r_index]\n",
    "                bucket_vs_all.append((buckets[l_index], vs_all))\n",
    "        return bucket_vs_all\n",
    " \n",
    "    def fit(self, X, y):\n",
    "\n",
    "        # Store the classes seen during fit\n",
    "        self.classes_ = unique_labels(y)\n",
    "        self.X_ = X\n",
    "        self.y_ = y\n",
    "        self.y_list_ = y.tolist()\n",
    "        self.y_counts_ = Counter(self.y_list_)\n",
    "        \n",
    "        self.buckets_ = self.get_buckets(self.y_counts_)\n",
    "        self.vs_all_ = self.get_vs_all(self.buckets_)\n",
    "\n",
    "        self.len_b_ = len(self.vs_all_)\n",
    "\n",
    "        self.vs_all_indicies_ = []\n",
    "        self.buckets_indicies_ = []\n",
    "        for bucket, rest in self.vs_all_:\n",
    "            bucket_indices = []\n",
    "            rest_indices = []\n",
    "            #for index, value in self.y_.iteritems():\n",
    "            \n",
    "            for index in range(len(self.y_list_)):\n",
    "                if self.y_list_[index] in bucket:\n",
    "                    bucket_indices.append(index)\n",
    "                if self.y_list_[index] in rest:\n",
    "                    rest_indices.append(index)\n",
    "                    \n",
    "            self.vs_all_indicies_.append(rest_indices)\n",
    "            self.buckets_indicies_.append(bucket_indices)\n",
    "\n",
    "        self.binary_clfs_ = [self.binary_clf(**self.binary_clf_params) for _ in range(self.len_b_)]\n",
    "        \n",
    "        for index, clf in enumerate(self.binary_clfs_):\n",
    "\n",
    "            fit_y = []\n",
    "            fit_x_index = []\n",
    "            for i in range(len(self.y_list_)):\n",
    "                if i in self.buckets_indicies_[index]:\n",
    "                    fit_y.append(0)\n",
    "                    fit_x_index.append(i)\n",
    "                if i in self.vs_all_indicies_[index]:\n",
    "                    fit_y.append(1)\n",
    "                    fit_x_index.append(i)\n",
    "                    \n",
    "            fit_x = self.X_[fit_x_index]\n",
    "            \n",
    "            clf.fit(fit_x, fit_y)\n",
    "\n",
    "        self.bucket_clfs_ = [self.bucket_clf(**self.bucket_clf_params) for _ in range(self.len_b_)]\n",
    "        \n",
    "        for index, clf in enumerate(self.bucket_clfs_):\n",
    "            \n",
    "            bucket_y = [y for i, y in enumerate(self.y_list_) if i in self.buckets_indicies_[index]]\n",
    "            bucket_x = self.X_[self.buckets_indicies_[index]]\n",
    "            \n",
    "            clf.fit(bucket_x, bucket_y)\n",
    "            \n",
    "        '''\n",
    "        judge_train = []\n",
    "\n",
    "        for index in range(self.len_b_):\n",
    "            \n",
    "            judge_train.append(self.binary_clfs_[index].predict(self.X_))\n",
    "            judge_train.append(self.bucket_clfs_[index].predict(self.X_))\n",
    "             \n",
    "        self.judge_train_t_ = np.array(judge_train).T\n",
    "\n",
    "        self.judge.fit(self.judge_train_t_, self.y_list_)\n",
    "        '''\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \n",
    "        '''\n",
    "        votes = []\n",
    "        \n",
    "        for index in range(self.len_b_):\n",
    "            votes.append(self.binary_clfs_[index].predict(X))\n",
    "            votes.append(self.bucket_clfs_[index].predict(X))\n",
    "\n",
    "        votes_t = np.array(votes).T \n",
    "        \n",
    "        print(list(votes_t))\n",
    "        \n",
    "        #return votes_t\n",
    "        preds = self.judge.predict(votes_t)\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        len_x = X.shape[0]\n",
    "        \n",
    "        preds = np.ones(len_x) * 12\n",
    "        \n",
    "        predicted = []\n",
    "        for index in range(self.len_b_):\n",
    "            \n",
    "            split = self.binary_clfs_[index].predict(X)\n",
    "\n",
    "            in_bucket = [i for i in range(len_x) if split[i] == 0 and i not in predicted]\n",
    "            preds[in_bucket] = self.bucket_clfs_[index].predict(X[in_bucket])\n",
    "            \n",
    "            predicted += in_bucket\n",
    "\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "train_x = vectorizer.fit_transform(train_X[corpus])\n",
    "validation_x = vectorizer.transform(validation_X[corpus])\n",
    "\n",
    "#validation_y = [1,1,3,4,2,3,5,6]\n",
    "#testx = ['abr haha jo', 'haha jo uff', 'terror jo haha', 'baluba joho', 'haha jodli', 'baluba total', '3', '3']\n",
    "#train_y = np.array([1,1,3,4,2,3,5,6])\n",
    "#train_x = vectorizer.fit_transform(testx)\n",
    "#validation_x = vectorizer.transform(testx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_bucket = EnsembleClassifier(LogisticRegression, {'solver': 'lbfgs'}, MultinomialNB, {'alpha': 0.001}, MultinomialNB(alpha=0.001))\n",
    "\n",
    "first_bucket.fit(train_x, train_y)\n",
    "preds = first_bucket.predict(validation_x)\n",
    "#print(preds)\n",
    "np.mean(preds == validation_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testy = Counter(train_y)\n",
    "\n",
    "testy_sum = sum(testy.values())\n",
    "\n",
    "buckets = []\n",
    "index = 0\n",
    "for c, v in testy.most_common():\n",
    "    if index == 0:\n",
    "        sum_left = testy_sum\n",
    "        sum_bucket = 0\n",
    "        new_bucket = []\n",
    "\n",
    "    new_bucket.append(c)\n",
    "    sum_bucket += v\n",
    "    sum_left -= v\n",
    "    \n",
    "    if sum_bucket >= sum_left:\n",
    "        buckets.append(new_bucket)\n",
    "        new_bucket = []\n",
    "        sum_bucket = 0\n",
    "    index += 1\n",
    "    if index == len(testy) and len(new_bucket) > 0:\n",
    "        buckets.append(new_bucket)\n",
    "        \n",
    "print(buckets)\n",
    "\n",
    "bucket_vs_all = []\n",
    "for l_index in range(len(buckets)):\n",
    "    vs_all = []\n",
    "    if l_index+1 < len(buckets):\n",
    "        for r_index in range(l_index+1, len(buckets)):\n",
    "            vs_all += buckets[r_index]\n",
    "        bucket_vs_all.append((buckets[l_index], vs_all))\n",
    "        \n",
    "print(bucket_vs_all)\n",
    "\n",
    "\n",
    "\n",
    "testy = train_y.tolist()\n",
    "vs_all_indicies = []\n",
    "for bucket, rest in bucket_vs_all:\n",
    "    bucket_indices = []\n",
    "    rest_indices = []\n",
    "    for index, value in train_y.iteritems():\n",
    "        if value in bucket:\n",
    "            bucket_indices.append(index)\n",
    "        if value in rest:\n",
    "            rest_indices.append(index)\n",
    "            \n",
    "    vs_all_indicies.append((bucket_indices, rest_indices))\n",
    "    \n",
    "#print(vs_all_indicies[-1][1])\n",
    "\n",
    "#print(train_X.loc[vs_all_indicies[-1][0]]['Lemma'])\n",
    "\n",
    "zero_index = train_y.loc[vs_all_indicies[-1][0]].index\n",
    "zeros = np.zeros(len(zero_index), dtype=int)\n",
    "zeros_y = pd.Series(zeros, index=zero_index)\n",
    "zero_x = train_X.loc[vs_all_indicies[-1][0]]\n",
    "                     \n",
    "one_index = train_y.loc[vs_all_indicies[-1][1]].index\n",
    "ones = np.ones(len(one_index), dtype=int)\n",
    "ones_y = pd.Series(ones, index=one_index)\n",
    "one_x = train_X.loc[vs_all_indicies[-1][1]]\n",
    "\n",
    "union_x = pd.concat([zero_x, one_x])\n",
    "union_y = pd.concat([zeros_y, ones_y])\n",
    "\n",
    "print(union_y)\n",
    "print(union_x)\n",
    "\n",
    "#binary_clfs = [MultinomialNB(**{'alpha': 0.001}) for _ in range(len(vs_all_indicies))]\n",
    "\n",
    "#for b_clf in binary_clfs:\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
